# üéÆ GU√çA DE USO DE GPU NVIDIA

## üéØ Resumen

El sistema ahora **detecta y usa autom√°ticamente tu GPU NVIDIA** si est√° disponible, con **l√≠mite configurable de VRAM**.

---

## ‚úÖ Configuraci√≥n Predeterminada

```env
USE_GPU=auto           # Detecci√≥n autom√°tica
GPU_MEMORY_LIMIT=2048  # L√≠mite de 2GB de VRAM
```

### ¬øQu√© hace esto?

- ‚úÖ **Detecta autom√°ticamente** si tienes GPU NVIDIA con CUDA
- ‚úÖ **Usa solo 2GB de VRAM** (deja espacio para otras aplicaciones)
- ‚úÖ **Falla gracefully** a CPU si hay problemas
- ‚úÖ **Funciona sin configuraci√≥n** adicional

---

## üõ†Ô∏è Configuraciones Comunes

### Caso 1: Tengo GPU Potente (6GB+ VRAM)

```env
USE_GPU=auto
GPU_MEMORY_LIMIT=4096  # Usar 4GB
WHISPER_MODEL=large    # Modelo m√°s preciso
```

### Caso 2: GPU Compartida (Gaming + Transcripci√≥n)

```env
USE_GPU=auto
GPU_MEMORY_LIMIT=2048  # Solo 2GB, deja resto libre
WHISPER_MODEL=medium
```

### Caso 3: Solo Tengo CPU (Sin GPU)

```env
USE_GPU=false          # Forzar CPU
WHISPER_MODEL=small    # Modelo m√°s ligero
```

### Caso 4: GPU de Laptop (4GB VRAM)

```env
USE_GPU=auto
GPU_MEMORY_LIMIT=3072  # Usar 3GB
WHISPER_MODEL=medium
```

### Caso 5: Quiero Usar Toda la GPU

```env
USE_GPU=auto
GPU_MEMORY_LIMIT=8192  # 8GB (ajusta seg√∫n tu GPU)
WHISPER_MODEL=large
```

---

## üìä Uso de VRAM por Modelo Whisper

| Modelo | VRAM Necesaria | GPU_MEMORY_LIMIT Recomendado |
|--------|----------------|------------------------------|
| tiny | ~500 MB | 1024 (1GB) |
| base | ~750 MB | 1024 (1GB) |
| small | ~1.5 GB | 2048 (2GB) |
| **medium** | **~2.5 GB** | **2048-3072 (2-3GB)** |
| large | ~5 GB | 6144 (6GB) |

---

## üöÄ Verificar si Est√° Usando GPU

Cuando ejecutes `.\run.ps1`, ver√°s en los logs:

### ‚úÖ GPU Detectada y Funcionando

```
üéÆ GPU detectada: NVIDIA GeForce RTX 3060
üìä VRAM total disponible: 12.00 GB
‚öôÔ∏è  L√≠mite configurado: 2048 MB (2.00 GB)
‚úÖ L√≠mite de VRAM aplicado: 2048MB = 16.7% de GPU
Dispositivo seleccionado: cuda
Cargando modelo Whisper (medium)...
‚úÖ Modelo cargado en GPU
üìä VRAM utilizada: 2.13 GB (reservada: 2.25 GB)
```

### üñ•Ô∏è Usando CPU

```
üñ•Ô∏è  GPU NVIDIA no detectada. Usando CPU.
Dispositivo seleccionado: cpu
Cargando modelo Whisper (medium)...
‚úÖ Modelo cargado en CPU
```

---

## ‚ö†Ô∏è Soluci√≥n de Problemas

### Error: "CUDA out of memory"

**Causa:** GPU sin suficiente VRAM libre.

**Soluciones:**
```env
# Opci√≥n 1: Aumentar l√≠mite (si tu GPU tiene m√°s VRAM)
GPU_MEMORY_LIMIT=4096

# Opci√≥n 2: Usar modelo m√°s peque√±o
WHISPER_MODEL=small
GPU_MEMORY_LIMIT=1024

# Opci√≥n 3: Forzar uso de CPU
USE_GPU=false
```

### GPU No Detectada (Teniendo NVIDIA)

**Verificar Docker con GPU:**
```powershell
# Ver si Docker detecta tu GPU
docker run --rm --gpus all nvidia/cuda:12.0.0-base-ubuntu22.04 nvidia-smi
```

**Si falla:**
1. Instalar [NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html)
2. Reiniciar Docker Desktop
3. Verificar en Docker Desktop ‚Üí Settings ‚Üí Resources ‚Üí GPU

### Sistema Lento con GPU

**Posibles causas:**
- L√≠mite de VRAM muy bajo (GPU hace swap)
- Modelo muy grande para tu GPU

**Soluci√≥n:**
```env
# Aumentar l√≠mite o usar CPU
GPU_MEMORY_LIMIT=3072  # Intentar con 3GB
# O
USE_GPU=false  # Usar CPU si es m√°s r√°pido
```

---

## üîç Comandos √ötiles

### Ver Uso de GPU en Tiempo Real

```powershell
# Abrir otro terminal y ejecutar:
docker exec -it audio-transcriber nvidia-smi

# Refrescar cada 2 segundos
docker exec -it audio-transcriber watch -n 2 nvidia-smi
```

### Ver Logs Detallados

```powershell
docker-compose logs -f audio-transcriber | Select-String "GPU|VRAM|cuda"
```

---

## üéØ Recomendaciones

### Para M√°xima Velocidad
```env
USE_GPU=auto
GPU_MEMORY_LIMIT=4096  # 4GB
WHISPER_MODEL=medium   # Balance velocidad/calidad
```

### Para M√°xima Calidad
```env
USE_GPU=auto
GPU_MEMORY_LIMIT=6144  # 6GB
WHISPER_MODEL=large    # Mejor precisi√≥n
```

### Para Uso Conservador
```env
USE_GPU=auto
GPU_MEMORY_LIMIT=2048  # Solo 2GB
WHISPER_MODEL=medium   # Buena calidad
```

### Para Laptops
```env
USE_GPU=auto
GPU_MEMORY_LIMIT=2048  # Conservador
WHISPER_MODEL=small    # Evita sobrecalentamiento
```

---

## üîß Configuraci√≥n Avanzada

### Usar GPU Espec√≠fica (Multi-GPU)

Edita `docker-compose.yml`:
```yaml
environment:
  - CUDA_VISIBLE_DEVICES=0  # GPU 0, 1, 2, etc.
```

### Monitorear Temperatura GPU

```powershell
# Requiere nvidia-smi
docker exec -it audio-transcriber nvidia-smi --query-gpu=temperature.gpu --format=csv,noheader
```

---

## ‚úÖ Checklist de GPU

Verifica que todo funciona:

- [ ] Docker Desktop tiene acceso a GPU habilitado
- [ ] `USE_GPU=auto` en `.env`
- [ ] `GPU_MEMORY_LIMIT` configurado seg√∫n tu GPU
- [ ] Logs muestran "GPU detectada" al ejecutar
- [ ] VRAM utilizada est√° dentro del l√≠mite
- [ ] Transcripci√≥n es m√°s r√°pida que con CPU

---

## üìö Recursos

- [NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/)
- [Docker GPU Support](https://docs.docker.com/config/containers/resource_constraints/#gpu)
- [Whisper GPU Requirements](https://github.com/openai/whisper#available-models-and-languages)

---

**√öltima actualizaci√≥n:** 19 de noviembre de 2025  
**Versi√≥n:** 1.0.0
